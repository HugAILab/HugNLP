python3 -m torch.distributed.launch --nproc_per_node=4 --master_port 6010 hugnlp_runner.py \
--model_name_or_path=/wjn/pre-trained-lm/roberta-base \
--data_dir=/wjn/nlp_trainer/pretrain_data/data/ \
--train_file=/wjn/nlp_trainer/pretrain_data/features/data.json \
--max_seq_length=512 \
--output_dir=./outputs/pretrain/kg/ \
--do_train \
--do_eval \
--per_device_train_batch_size=22 \
--per_device_eval_batch_size=22 \
--evaluation_strategy=steps \
--eval_steps=1000 \
--gradient_accumulation_steps=2 \
--learning_rate=1e-05 \
--logging_steps=10000000 \
--save_steps=1000 \
--save_total_limit=5 \
--num_train_epochs=3 \
--report_to=none \
--task_name=en_wiki_kpplm \
--task_type=kpplm_roberta \
--exp_name=kp-plm \
--warmup_steps=10000 \
--model_type=roberta \
--ignore_data_skip \
--remove_unused_columns=False \
--load_best_model_at_end \
--metric_for_best_model=acc \
--fp16 \
--max_eval_samples=30000 \
--overwrite_output_dir \
--dataloader_num_workers 2
